{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca209422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import pygame\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6323bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eddc73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_space,128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, action_space)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = torch.relu(self.fc1(input))\n",
    "        input = torch.relu(self.fc2(input))\n",
    "        return self.fc3(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37adca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training neural networks, we split the data into batches.\n",
    "# To improve the training, we need to remove the \"correlation\" between game states\n",
    "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
    "# states at random which reduces the correlation.\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fe4d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(obs_dim, n_actions).to(device)\n",
    "target_net = DQN(obs_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "buffer = ExperienceBuffer(10000)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "sync_target_steps = 100\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "num_episodes = 500\n",
    "epsilon_decay = (epsilon_start - epsilon_end) / num_episodes\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    epsilon = epsilon_start - episode*epsilon_decay\n",
    "    while not done and epsilon > epsilon_end:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(obs, dtype=torch.float32, device = device).unsqueeze(0)\n",
    "                q_values = policy_net(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        buffer.push(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_done += 1\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            states = np.array(states, dtype=np.float32)\n",
    "            states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64, device = device).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device = device).unsqueeze(1)\n",
    "            next_states = np.array(next_states, dtype=np.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32, device = device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                max_next_q = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if steps_done % sync_target_steps == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6493553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cartpole_model(model, episodes=20, render=True):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8643afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 500.0\n",
      "Episode 2: Reward = 215.0\n",
      "Episode 3: Reward = 219.0\n",
      "Episode 4: Reward = 207.0\n",
      "Episode 5: Reward = 233.0\n",
      "Episode 6: Reward = 221.0\n",
      "Episode 7: Reward = 215.0\n",
      "Episode 8: Reward = 500.0\n",
      "Episode 9: Reward = 248.0\n",
      "Episode 10: Reward = 282.0\n",
      "Average reward over 10 episodes: 284.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_cartpole_model(policy_net, episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c4213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
    "\n",
    "    def __init__(self, size=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.cell_size = 30\n",
    "        self.screen_size = self.size * self.cell_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
    "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size), dtype=np.uint8)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "\n",
    "        self.snake = deque()\n",
    "        self.food = None\n",
    "        self.direction = [1, 0]\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, seed=None, options=None):           \n",
    "        super().reset(seed=seed)\n",
    "        self.snake.clear()\n",
    "        mid = self.size // 2\n",
    "        self.snake.appendleft([mid, mid])\n",
    "        self.direction = [1, 0]\n",
    "        self._place_food()\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_init()\n",
    "\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "    \n",
    "        if action == 0 and self.direction != [-1, 0]: self.direction = [1, 0]\n",
    "        elif action == 1 and self.direction != [0, 1]: self.direction = [0, -1]\n",
    "        elif action == 2 and self.direction != [1, 0]: self.direction = [-1, 0]\n",
    "        elif action == 3 and self.direction != [0, -1]: self.direction = [0, 1]\n",
    "    \n",
    "        head = self.snake[0]\n",
    "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
    "    \n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        if not (0 <= new_head[0] < self.size and 0 <= new_head[1] < self.size):\n",
    "            done = True\n",
    "            reward = -1\n",
    "        else:\n",
    "            body_to_check = list(self.snake)[:-1] if new_head != self.food else list(self.snake)\n",
    "            if new_head in body_to_check:\n",
    "                done = True\n",
    "                reward = -1\n",
    "    \n",
    "        if not done:\n",
    "            self.snake.appendleft(new_head)\n",
    "            if new_head == self.food:\n",
    "                reward = 1\n",
    "                self._place_food()\n",
    "            else:\n",
    "                self.snake.pop()\n",
    "        else:\n",
    "            distance = np.linalg.norm(np.array(new_head) - np.array(self.food))\n",
    "            reward = -distance * 0.01\n",
    "    \n",
    "        obs = self._get_obs()\n",
    "    \n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "    \n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "       head = self.snake[0]\n",
    "       new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
    "       return {\"agent\": new_head, \"target\": self.food}\n",
    "\n",
    "    def _place_food(self):\n",
    "        positions = set(tuple(p) for p in self.snake)\n",
    "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
    "        self.food = list(random.choice(empty)) if empty else None\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            self._render_init()\n",
    "\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (0, 255, 0),\n",
    "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "        if self.food:\n",
    "            fx, fy = self.food\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (255, 0, 0),\n",
    "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def _render_init(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen:\n",
    "            pygame.quit()\n",
    "            self.screen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d3ffb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_obs(obs):\n",
    "    return np.array(list(obs[\"agent\"]) + list(obs[\"target\"]), dtype=np.float32)\n",
    "\n",
    "env = SnakeGame(size=10, render_mode=None)\n",
    "obs_dim = len(flatten_obs(env.reset()[0]))\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(obs_dim, n_actions)\n",
    "target_net = DQN(obs_dim, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "buffer = ExperienceBuffer(10000)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "sync_target_steps = 100\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = (epsilon_start - epsilon_end)/num_episodes\n",
    "steps_done = 0\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 200\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    obs = flatten_obs(obs)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    epsilon = max(epsilon_end, epsilon_start - episode * epsilon_decay)\n",
    "    while not done and steps_done < max_steps_per_episode:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "                q_values = policy_net(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        next_obs = flatten_obs(next_obs)\n",
    "        buffer.push(obs, action, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_done += 1\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                max_next_q = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if steps_done % sync_target_steps == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0e10565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
    "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs = flatten_obs(obs)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            next_obs, reward, done, _, _ = env.step(action)\n",
    "            next_obs = flatten_obs(next_obs)\n",
    "            buffer.push(obs, action, reward, next_obs, done)\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "    \n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a7eb581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -0.09433981132056603\n",
      "Episode 2: Reward = -0.1503329637837291\n",
      "Episode 3: Reward = -0.11401754250991379\n",
      "Episode 4: Reward = -0.08602325267042626\n",
      "Episode 5: Reward = -0.13341664064126335\n",
      "Episode 6: Reward = -0.09055385138137417\n",
      "Episode 7: Reward = -0.0316227766016838\n",
      "Episode 8: Reward = -0.09055385138137417\n",
      "Episode 9: Reward = -0.12083045973594572\n",
      "Episode 10: Reward = -0.13601470508735444\n",
      "Average reward over 10 episodes: -0.10477058551136309\n"
     ]
    }
   ],
   "source": [
    "evaluate_snake_model(policy_net, episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4224fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChaseEscapeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 15}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dt = 0.1\n",
    "        self.max_speed = 0.4\n",
    "        self.agent_radius = 0.05\n",
    "        self.target_radius = 0.05\n",
    "        self.chaser_radius = 0.07\n",
    "        self.chaser_speed = 0.03\n",
    "\n",
    "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(8,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_size = 500\n",
    "        self.np_random = None\n",
    "\n",
    "        if render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
    "        while True:\n",
    "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
    "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
    "                return pos\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.agent_pos = self.sample_pos()\n",
    "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate([ self.agent_pos, self.agent_vel, self.target_pos, self.chaser_pos ])\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: Add reward scheme\n",
    "        # 1) Try to make the agent stay within bounds\n",
    "        # 2) The agent shouldn't idle around\n",
    "        # 3) The agent should go for the reward\n",
    "        # 4) The agent should avoid the chaser\n",
    "        \n",
    "        accel = (np.array(action) - 1) * 0.1\n",
    "        self.agent_vel += accel\n",
    "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
    "        self.agent_pos += self.agent_vel * self.dt\n",
    "        self.agent_pos = np.clip(self.agent_pos, -1, 1)\n",
    "\n",
    "        direction = self.agent_pos - self.chaser_pos\n",
    "        norm = np.linalg.norm(direction)\n",
    "        if norm > 1e-5:\n",
    "            self.chaser_pos += self.chaser_speed * direction / norm\n",
    "\n",
    "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
    "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
    "\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "\n",
    "        if dist_to_target < self.agent_radius + self.target_radius:\n",
    "            reward += 3.0\n",
    "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "\n",
    "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
    "            reward -= 1.0\n",
    "            terminated = True\n",
    "\n",
    "        reward -= 0.01 * np.linalg.norm(self.agent_vel)\n",
    "\n",
    "        reward -= 0.5 * np.exp(-dist_to_chaser)\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, self._get_info()\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        def to_screen(p):\n",
    "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
    "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
    "            return x, y\n",
    "\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "def close(self):\n",
    "    if self.render_mode == \"human\":\n",
    "        pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9eca1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = ChaseEscapeEnv(render_mode=None)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.nvec.prod()\n",
    "\n",
    "def flatten_action(action):\n",
    "    return np.unravel_index(action, (3, 3))\n",
    "\n",
    "policy_net = DQN(obs_dim, n_actions).to(device)\n",
    "target_net = DQN(obs_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "buffer = ExperienceBuffer(10000)\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "sync_target_steps = 100\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "num_episodes = 500\n",
    "epsilon_decay = (epsilon_start - epsilon_end) / num_episodes\n",
    "steps_done = 0\n",
    "max_steps_per_episode = 300\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    epsilon = max(epsilon_end, epsilon_start - episode * epsilon_decay)\n",
    "    step = 0\n",
    "    while not done and step < max_steps_per_episode:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action_idx = np.random.randint(n_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_values = policy_net(state)\n",
    "                action_idx = torch.argmax(q_values, dim=1).item()\n",
    "        action = flatten_action(action_idx)\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        buffer.push(obs, action_idx, reward, next_obs, done)\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "        steps_done += 1\n",
    "        step += 1\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "            actions = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "            q_values = policy_net(states).gather(1, actions)\n",
    "            with torch.no_grad():\n",
    "                max_next_q = target_net(next_states).max(1, keepdim=True)[0]\n",
    "                target_q = rewards + gamma * max_next_q * (1 - dones)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(q_values, target_q)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if steps_done % sync_target_steps == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4af81cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chaseescape_model(model, episodes=10, render=True):\n",
    "    env = ChaseEscapeEnv(render_mode=\"human\" if render else None)\n",
    "    model.eval()\n",
    "    rewards = []\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action_idx = torch.argmax(q_values, dim=1).item()\n",
    "            action = (action_idx // 3, action_idx % 3)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminated or truncated\n",
    "            steps += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward:.2f}  Steps = {steps}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8172ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -5.33  Steps = 13\n",
      "Episode 2: Reward = -16.02  Steps = 43\n",
      "Episode 3: Reward = -7.03  Steps = 24\n",
      "Episode 4: Reward = -14.32  Steps = 66\n",
      "Episode 5: Reward = -6.83  Steps = 17\n",
      "Episode 6: Reward = -5.91  Steps = 19\n",
      "Episode 7: Reward = -11.47  Steps = 50\n",
      "Episode 8: Reward = -6.07  Steps = 16\n",
      "Episode 9: Reward = -4.85  Steps = 12\n",
      "Episode 10: Reward = -13.26  Steps = 58\n",
      "Average reward over 10 episodes: -9.11\n"
     ]
    }
   ],
   "source": [
    "evaluate_chaseescape_model(policy_net, episodes=10, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
